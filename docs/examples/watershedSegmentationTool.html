<!DOCTYPE html>
<html class="h-100 overflow-hidden">
  <head>
    <meta http-equiv="Content-type" content="text/html" charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <style>
      .loading-text {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        color: #333;
        font-weight: bold;
      }
      #buttonBar {
        display: flex;
        gap: 10px;
        margin-bottom: 20px;
      }

      .button {
        background-color: #000000;
        color: #f0f0f0;
        padding: 10px; /* Adjust the padding to make the buttons bigger */
        font-size: 16px; /* Adjust the font size for better visibility */
      }

      .manualInput {
        width: 150px; /* Adjust the width of manual input for better visibility */
        padding: 8px; /* Adjust the padding for better alignment */
        font-size: 14px; /* Adjust the font size for better visibility */
      }

      .loader {
        border: 8px solid #f3f3f3;
        border-top: 8px solid #3498db;
        border-radius: 50%;
        width: 30px;
        height: 30px;
        animation: spin 1s linear infinite;
        margin: 3px;
      }
    </style>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN"
      crossorigin="anonymous"
    />
    <link
      rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/vs2015.min.css"
    />
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
    <script>
      hljs.highlightAll();
    </script>
    <title>Watershed segmentation implemented in OpenCV.js</title>
  </head>

  <body class="h-100" style="background-color: #000000">
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL"
      crossorigin="anonymous"
    ></script>
    <div class="row h-100 d-flex align-items-center justify-content-center">
      <div id="buttonBar">
        <div class="loader" id="loader" style="display: none"></div>

        <button id="toggleButton">Current Image</button>
        <input
          type="number"
          class="manualInput"
          id="masksNumber"
          placeholder="Masks Number"
        />
        <input
          type="number"
          class="manualInput"
          id="startImage"
          placeholder="startImage"
          style="display: none"
        />
        <input
          type="number"
          class="manualInput"
          id="endImage"
          placeholder="endImage"
          style="display: none"
        />
      </div>
      <div id="viewer" class="col-8 h-100" style="background-color: black">
        <p id="info " style="position: absolute; color: white">
          <b>ctrl+mouse wheel to change brush radius</b><br /><b
            >click to activate watershed segmentation of features with greyscale
            value of interest</b
          ><br />
          <b>ctrl+click for Label Eraser:</b> erases selected label<br />
          <b>alt+click for LabelPicker: </b>allows to pick label, click again to
          apply picked label<br />
          <b>shift+click+drag for Manual Eraser</b>
        </p>
      </div>

      <div class="col-4 h-100">
        <pre class="h-100">
       
        <code class="typescript" style="background-color: #000000">
          <p style="font-size:0.7vw;">
            let demoFiles = [];
      let counter = 0;

      const getDemoFileNames = function () {
        let demoFileList = [];
        for (let i = 1; i < 25; i++) {
          let filename = "anon" + i;
          demoFileList.push(filename);
        }
        return demoFileList;
      };

      // init all
      larvitar.initializeImageLoader();
      larvitar.initializeCSTools();
      larvitar.store.initialize();
      larvitar.store.addViewport("viewer");
      larvitar.registerExternalTool("Test", WatershedSegmentationTool);

      async function createFile(fileName, cb) {
        let response = await fetch("./demo/" + fileName);
        let data = await response.blob();
        let file = new File([data], fileName);
        demoFiles.push(file);
        counter++;
        if (counter == 24) {
          cb();
        }
      }

      function renderSerie() {
        larvitar.resetLarvitarManager();
        larvitar
          .readFiles(demoFiles)
          .then(seriesStack => {
            // render the first series of the study
            let seriesId = _.keys(seriesStack)[0];
            let serie = seriesStack[seriesId];
            larvitar.renderImage(serie, "viewer").then(() => {
              console.log("Image has been rendered");
            });
            // optionally cache the series
            larvitar.populateLarvitarManager(seriesId, serie);
            larvitar.cacheImages(serie, function (resp) {
              if (resp.loading == 100) {
                let cache = larvitar.cornerstone.imageCache;
                console.log(
                  "Cache size: ",
                  cache.getCacheInfo().cacheSizeInBytes / 1e6,
                  "Mb"
                );
              }
            });
            larvitar.addTool("Test");
            larvitar.setToolActive("Test");
          })
          .catch(err => console.error(err));
      }

      let demoFileList = getDemoFileNames();
      _.each(demoFileList, function (demoFile) {
        createFile(demoFile, renderSerie);
      });

/*Hahn and Peitgen [2000] extracted the brain with a single watershed transform from MRI data. 
Also, the cerebral ventricles were reliably segmented with minimal interaction. 
Hahn and Peitgen [2003] demonstrated the application to the challenging problem of
delineating individual bones in the human wrist (see Fig. 4.17). 
Kuhnigk et al. [2003] employed the above-described variant of the watershed segmentation 
to the delineation of lung lobes in CT data. 
Ray et al. [2008] used the iterative watershed transform 
for hepatic tumor segmentation (and volumetry).*/


        </p>    
    </code>
  </pre>
      </div>
    </div>
    <script src="../../dist/larvitar.js"></script>
    <script src="../../node_modules/cornerstone-core/dist/cornerstone.js"></script>
    <script src="../../node_modules/cornerstone-wado-image-loader/dist/cornerstoneWADOImageLoader.bundle.min.js"></script>
    <script src="../../node_modules/dicom-parser/dist/dicomParser.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/lodash@4.17.21/lodash.min.js"></script>

    <script>
      function onOpenCvReady() {
        console.log("OpenCV Ready", cv);
      }
    </script>
    <script
      src="https://docs.opencv.org/4.5.4/opencv.js"
      onload="onOpenCvReady();"
      type="text/javascript"
    ></script>
    <script>
      const loader = document.getElementById("loader");
      let rotation = 0;

      function rotateLoader() {
        loader.style.display = "block";

        if (larvitar.DEFAULT_TOOLS["WSToggle"].configuration.onload) {
          rotation += 10; // You can adjust the rotation speed
          loader.style.transform = `rotate(${rotation}deg)`;
          requestAnimationFrame(rotateLoader);
        } else {
          loader.style.display = "none";
        }
      }
      loader.style.display = "none";
      //rotateLoader();
    </script>

    <script>
      function triggerHandleToggle() {
        let isMultiImage =
          larvitar.DEFAULT_TOOLS["WSToggle"].configuration.multiImage === false
            ? true
            : false;
        larvitar.DEFAULT_TOOLS["WSToggle"].configuration.multiImage =
          isMultiImage;
        toggleButton.innerHTML =
          isMultiImage === true ? "Multi Image" : "Current Image";
        startInput.style.display = isMultiImage === true ? "block" : "none";
        endInput.style.display = isMultiImage === true ? "block" : "none";
      }
      const masksNumberInput = document.getElementById("masksNumber");
      // Add an input event listener
      masksNumberInput.addEventListener("input", function (event) {
        // Update the variable with the input value
        larvitar.DEFAULT_TOOLS["WSToggle"].configuration.masksNumber =
          event.target.value;
      });
      const startInput = document.getElementById("startImage");
      // Add an input event listener
      startInput.addEventListener("input", function (event) {
        // Update the variable with the input value
        console.log("Start", event.target.value);
        larvitar.DEFAULT_TOOLS["WSToggle"].configuration.startIndex =
          event.target.value;
      });
      const endInput = document.getElementById("endImage");
      // Add an input event listener
      endInput.addEventListener("input", function (event) {
        // Update the variable with the input value
        console.log("end", event.target.value);
        larvitar.DEFAULT_TOOLS["WSToggle"].configuration.endIndex =
          event.target.value;
      });
      // Attach click event to the button
      const toggleButton = document.getElementById("toggleButton");
      toggleButton.addEventListener("click", triggerHandleToggle);

      let demoFiles = [];
      let counter = 0;
      const getDemoFileNames = function () {
        let demoFileList = [];
        for (let i = 101; i < 125; i++) {
          let filename = "I" + i;
          demoFileList.push(filename);
        }
        return demoFileList;
      };
      // init all
      larvitar.initializeImageLoader();
      larvitar.initializeCSTools();
      larvitar.store.initialize();
      larvitar.store.addViewport("viewer");
      let element = document.getElementById("viewer");
      element.addEventListener("click", rotateLoader);
      larvitar.registerNRRDImageLoader();
      larvitar.initSegmentationModule();
      const cornerstone = larvitar.cornerstone;
      async function createFile(fileName, cb) {
        let response = await fetch("./demo/covid/" + fileName);
        let data = await response.blob();
        let file = new File([data], fileName);
        demoFiles.push(file);
        counter++;
        if (counter == 24) {
          cb();
        }
      }

      async function renderSerie() {
        larvitar.resetLarvitarManager();
        larvitar
          .readFiles(demoFiles)
          .then((seriesStack) => {
            console.log(seriesStack);
            // render the first series of the study
            let seriesId = _.keys(seriesStack)[0];
            let serie = seriesStack[seriesId];
            larvitar.renderImage(serie, "viewer").then(() => {
              console.log("Image has been rendered");
            });
            // optionally cache the series
            larvitar.populateLarvitarManager(seriesId, serie);

            larvitar
              .cacheImages(serie, function (resp) {
                if (resp.loading == 100) {
                  let cache = larvitar.cornerstone.imageCache;
                  console.log(
                    "Cache size: ",
                    cache.getCacheInfo().cacheSizeInBytes / 1e6,
                    "Mb"
                  );
                }
              })
              .then(() => {
                setTimeout(function () {
                  console.log("Cache has been loaded. Calling loadMasks.");
                  larvitar.addDefaultTools();
                  loadMasks();
                }, 3000);
              });
          })
          .catch((err) => console.error(err));
      }

      async function loadMasks() {
        console.log("loadMasks() called");
        let data = new Int16Array(768 * 768 * 24); //fix dimensions based on loaded images dimensions
        let properties = {
          // color: "#00ff00",
          opacity: 0.2,
          labelId: 0,
        };
        // add to viewport
        await larvitar
          .addSegmentationMask(properties, data, "viewer")
          .then(() => {
            // activate brush on this labelmap
            larvitar.setActiveLabelmap(0, "viewer");
            larvitar.setToolActive("WSToggle");
          });
      }

      let demoFileList = getDemoFileNames();
      _.each(demoFileList, function (demoFile) {
        createFile(demoFile, renderSerie);
      });
      /*Hahn and Peitgen [2000] extracted the brain with a single watershed transform from MRI data.
      Also, the cerebral ventricles were reliably segmented with minimal interaction.
      Hahn and Peitgen [2003] demonstrated the application to the challenging problem of
      delineating individual bones in the human wrist (see Fig. 4.17).
      Kuhnigk et al. [2003] employed the above-described variant of the watershed segmentation
      to the delineation of lung lobes in CT data.
      Ray et al. [2008] used the iterative watershed transform
      for hepatic tumor segmentation (and volumetry).*/
    </script>
  </body>
</html>
