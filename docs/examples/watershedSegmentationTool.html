<!DOCTYPE html>
<html class="h-100 overflow-hidden">
  <head>
    <meta http-equiv="Content-type" content="text/html" charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <style>
      .loading-text {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        color: #333;
        font-weight: bold;
      }
      #buttonBar {
        gap: 10px;
        margin-bottom: 20px;
        position: relative;
      }

      .button {
        background-color: #000000;
        color: #f0f0f0;
        padding: 10px; /* Adjust the padding to make the buttons bigger */
        font-size: 16px; /* Adjust the font size for better visibility */
      }

      .manualInput {
        width: 150px; /* Adjust the width of manual input for better visibility */
        padding: 8px; /* Adjust the padding for better alignment */
        font-size: 14px; /* Adjust the font size for better visibility */
      }
    </style>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN"
      crossorigin="anonymous"
    />
    <link
      rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/vs2015.min.css"
    />
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
    <script>
      hljs.highlightAll();
    </script>
    <title>Watershed segmentation implemented in OpenCV.js</title>
  </head>

  <body class="h-100" style="background-color: #000000">
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL"
      crossorigin="anonymous"
    ></script>
    <div class="row h-100">
      <div id="buttonBar">
        <button
          id="toggleButton"
          class="btn btn-primary"
          style="margin-left: 600px"
        >
          Current Image
        </button>
        <input
          type="number"
          class="manualInput"
          id="masksNumber"
          placeholder="Masks Number"
        />
        <input
          type="number"
          class="manualInput"
          id="startImage"
          placeholder="startImage"
          style="display: none"
        />
        <input
          type="number"
          class="manualInput"
          id="endImage"
          placeholder="endImage"
          style="display: none"
        />
        <button id="loaderDiv" class="btn btn-primary" type="button" disabled>
          <span
            id="loader"
            class="spinner-border spinner-border-sm"
            role="status"
            aria-hidden="true"
          ></span>
          Loading...
        </button>
      </div>
      <div id="viewer" class="col-8 h-100" style="background-color: black">
        <p id="info " style="position: absolute; color: white">
          <b>ctrl+mouse wheel to change brush radius</b><br /><b
            >click to activate watershed segmentation of features with greyscale
            value of interest</b
          ><br />
          <b>ctrl+click for Label Eraser:</b> erases selected label<br />
          <b>alt+click for LabelPicker: </b>allows to pick label, click again to
          apply picked label<br />
          <b>shift+click+drag for Manual Eraser</b>
        </p>
      </div>

      <div class="col-4 h-100">
        <pre class="h-100">
       
        <code class="typescript" style="background-color: #000000">
          <p style="font-size:0.7vw;">
            function triggerHandleToggle() {
              let isMultiImage =
                larvitar.DEFAULT_TOOLS["WSToggle"].configuration.multiImage === false
                  ? true
                  : false;
              larvitar.DEFAULT_TOOLS["WSToggle"].configuration.multiImage =
                isMultiImage;
              toggleButton.innerHTML =
                isMultiImage === true ? "Multi Image" : "Current Image";
              if (isMultiImage) {
                startInput.style.removeProperty("display");
                endInput.style.removeProperty("display");
              } else {
                startInput.style.display = "none";
                endInput.style.display = "none";
              }
            }
            const masksNumberInput = document.getElementById("masksNumber");
            // Add an input event listener
            masksNumberInput.addEventListener("input", function (event) {
              // Update the variable with the input value
              larvitar.DEFAULT_TOOLS["WSToggle"].configuration.masksNumber =
                event.target.value;
            });
            const startInput = document.getElementById("startImage");
            // Add an input event listener
            startInput.addEventListener("input", function (event) {
              // Update the variable with the input value
              console.log("Start", event.target.value);
              larvitar.DEFAULT_TOOLS["WSToggle"].configuration.startIndex =
                event.target.value;
            });
            const endInput = document.getElementById("endImage");
            // Add an input event listener
            endInput.addEventListener("input", function (event) {
              // Update the variable with the input value
              console.log("end", event.target.value);
              larvitar.DEFAULT_TOOLS["WSToggle"].configuration.endIndex =
                event.target.value;
            });
            // Attach click event to the button
            const toggleButton = document.getElementById("toggleButton");
            toggleButton.addEventListener("click", triggerHandleToggle);
      
            let demoFiles = [];
            let counter = 0;
            const getDemoFileNames = function () {
              let demoFileList = [];
              for (let i = 101; i < 125; i++) {
                let filename = "I" + i;
                demoFileList.push(filename);
              }
              return demoFileList;
            };
            // init all
            larvitar.initializeImageLoader();
            larvitar.initializeCSTools();
            larvitar.store.initialize();
            larvitar.store.addViewport("viewer");
            let element = document.getElementById("viewer");
            element.addEventListener("click", rotateLoader);
            larvitar.registerNRRDImageLoader();
            larvitar.initSegmentationModule();
            const cornerstone = larvitar.cornerstone;
            async function createFile(fileName, cb) {
              let response = await fetch("./demo/covid/" + fileName);
              let data = await response.blob();
              let file = new File([data], fileName);
              demoFiles.push(file);
              counter++;
              if (counter == 24) {
                cb();
              }
            }
      
            async function renderSerie() {
              larvitar.resetLarvitarManager();
              larvitar
                .readFiles(demoFiles)
                .then(seriesStack => {
                  console.log(seriesStack);
                  // render the first series of the study
                  let seriesId = _.keys(seriesStack)[0];
                  let serie = seriesStack[seriesId];
                  larvitar.renderImage(serie, "viewer").then(() => {
                    console.log("Image has been rendered");
                  });
                  // optionally cache the series
                  larvitar.populateLarvitarManager(seriesId, serie);
      
                  larvitar
                    .cacheImages(serie, function (resp) {
                      if (resp.loading == 100) {
                        let cache = larvitar.cornerstone.imageCache;
                        console.log(
                          "Cache size: ",
                          cache.getCacheInfo().cacheSizeInBytes / 1e6,
                          "Mb"
                        );
                      }
                    })
                    .then(() => {
                      setTimeout(function () {
                        console.log("Cache has been loaded. Calling loadMasks.");
                        larvitar.addDefaultTools();
                        loadMasks();
                      }, 3000);
                    });
                })
                .catch(err => console.error(err));
            }
      
            async function loadMasks() {
              console.log("loadMasks() called");
              let data = new Int16Array(768 * 768 * 24); //fix dimensions based on loaded images dimensions
              let properties = {
                // color: "#00ff00",
                opacity: 0.2,
                labelId: 0
              };
              // add to viewport
              await larvitar
                .addSegmentationMask(properties, data, "viewer")
                .then(() => {
                  // activate brush on this labelmap
                  larvitar.setActiveLabelmap(0, "viewer");
                  larvitar.setToolActive("WSToggle");
                });
            }
      
            let demoFileList = getDemoFileNames();
            _.each(demoFileList, function (demoFile) {
              createFile(demoFile, renderSerie);
            });
            /*Hahn and Peitgen [2000] extracted the brain with a single watershed transform from MRI data.
            Also, the cerebral ventricles were reliably segmented with minimal interaction.
            Hahn and Peitgen [2003] demonstrated the application to the challenging problem of
            delineating individual bones in the human wrist (see Fig. 4.17).
            Kuhnigk et al. [2003] employed the above-described variant of the watershed segmentation
            to the delineation of lung lobes in CT data.
            Ray et al. [2008] used the iterative watershed transform
            for hepatic tumor segmentation (and volumetry).*/

        </p>    
    </code>
  </pre>
      </div>
    </div>
    <script src="../../dist/larvitar.js"></script>
    <script src="../../node_modules/cornerstone-core/dist/cornerstone.js"></script>
    <script src="../../node_modules/cornerstone-wado-image-loader/dist/cornerstoneWADOImageLoader.bundle.min.js"></script>
    <script src="../../node_modules/dicom-parser/dist/dicomParser.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/lodash@4.17.21/lodash.min.js"></script>

    <script>
      function onOpenCvReady() {
        console.log("OpenCV Ready", cv);
      }
    </script>
    <script
      src="https://docs.opencv.org/4.5.4/opencv.js"
      onload="onOpenCvReady();"
      type="text/javascript"
    ></script>
    <script>
      const loader = document.getElementById("loader");
      const loaderDiv = document.getElementById("loaderDiv");
      let rotation = 0;

      function rotateLoader() {
        loaderDiv.style.flexGrow = 0;
        if (loaderDiv.innerHTML === "Loaded") {
          loaderDiv.innerHTML =
            '<span id="loader" class="spinner-border spinner-border-sm" role="status" aria-hidden="true"></span> Loading...';
        }

        loader.style.removeProperty("display");

        if (larvitar.DEFAULT_TOOLS["WSToggle"].configuration.onload) {
          requestAnimationFrame(rotateLoader);
        } else {
          loader.style.display = "none";
          loaderDiv.innerHTML = "Loaded";
        }
      }
      loader.style.display = "none";
      loaderDiv.innerHTML = "Loaded";

      //rotateLoader();
    </script>

    <script>
      function triggerHandleToggle() {
        let isMultiImage =
          larvitar.DEFAULT_TOOLS["WSToggle"].configuration.multiImage === false
            ? true
            : false;
        larvitar.DEFAULT_TOOLS["WSToggle"].configuration.multiImage =
          isMultiImage;
        toggleButton.innerHTML =
          isMultiImage === true ? "Multi Image" : "Current Image";
        if (isMultiImage) {
          startInput.style.removeProperty("display");
          endInput.style.removeProperty("display");
        } else {
          startInput.style.display = "none";
          endInput.style.display = "none";
        }
      }
      const masksNumberInput = document.getElementById("masksNumber");
      // Add an input event listener
      masksNumberInput.addEventListener("input", function (event) {
        // Update the variable with the input value
        larvitar.DEFAULT_TOOLS["WSToggle"].configuration.masksNumber =
          event.target.value;
      });
      const startInput = document.getElementById("startImage");
      // Add an input event listener
      startInput.addEventListener("input", function (event) {
        // Update the variable with the input value
        console.log("Start", event.target.value);
        larvitar.DEFAULT_TOOLS["WSToggle"].configuration.startIndex =
          event.target.value;
      });
      const endInput = document.getElementById("endImage");
      // Add an input event listener
      endInput.addEventListener("input", function (event) {
        // Update the variable with the input value
        console.log("end", event.target.value);
        larvitar.DEFAULT_TOOLS["WSToggle"].configuration.endIndex =
          event.target.value;
      });
      // Attach click event to the button
      const toggleButton = document.getElementById("toggleButton");
      toggleButton.addEventListener("click", triggerHandleToggle);

      let demoFiles = [];
      let counter = 0;
      const getDemoFileNames = function () {
        let demoFileList = [];
        for (let i = 101; i < 125; i++) {
          let filename = "I" + i;
          demoFileList.push(filename);
        }
        return demoFileList;
      };
      // init all
      larvitar.initializeImageLoader();
      larvitar.initializeCSTools();
      larvitar.store.initialize();
      larvitar.store.addViewport("viewer");
      let element = document.getElementById("viewer");
      element.addEventListener("click", rotateLoader);
      larvitar.registerNRRDImageLoader();
      larvitar.initSegmentationModule();
      const cornerstone = larvitar.cornerstone;
      async function createFile(fileName, cb) {
        let response = await fetch("./demo/covid/" + fileName);
        let data = await response.blob();
        let file = new File([data], fileName);
        demoFiles.push(file);
        counter++;
        if (counter == 24) {
          cb();
        }
      }

      async function renderSerie() {
        larvitar.resetLarvitarManager();
        larvitar
          .readFiles(demoFiles)
          .then(seriesStack => {
            console.log(seriesStack);
            // render the first series of the study
            let seriesId = _.keys(seriesStack)[0];
            let serie = seriesStack[seriesId];
            larvitar.renderImage(serie, "viewer").then(() => {
              console.log("Image has been rendered");
            });
            // optionally cache the series
            larvitar.populateLarvitarManager(seriesId, serie);

            larvitar
              .cacheImages(serie, function (resp) {
                if (resp.loading == 100) {
                  let cache = larvitar.cornerstone.imageCache;
                  console.log(
                    "Cache size: ",
                    cache.getCacheInfo().cacheSizeInBytes / 1e6,
                    "Mb"
                  );
                }
              })
              .then(() => {
                setTimeout(function () {
                  console.log("Cache has been loaded. Calling loadMasks.");
                  larvitar.addDefaultTools();
                  loadMasks();
                }, 3000);
              });
          })
          .catch(err => console.error(err));
      }

      async function loadMasks() {
        console.log("loadMasks() called");
        let data = new Int16Array(768 * 768 * 24); //fix dimensions based on loaded images dimensions
        let properties = {
          // color: "#00ff00",
          opacity: 0.2,
          labelId: 0
        };
        // add to viewport
        await larvitar
          .addSegmentationMask(properties, data, "viewer")
          .then(() => {
            // activate brush on this labelmap
            larvitar.setActiveLabelmap(0, "viewer");
            larvitar.setToolActive("WSToggle");
          });
      }

      let demoFileList = getDemoFileNames();
      _.each(demoFileList, function (demoFile) {
        createFile(demoFile, renderSerie);
      });
      /*Hahn and Peitgen [2000] extracted the brain with a single watershed transform from MRI data.
      Also, the cerebral ventricles were reliably segmented with minimal interaction.
      Hahn and Peitgen [2003] demonstrated the application to the challenging problem of
      delineating individual bones in the human wrist (see Fig. 4.17).
      Kuhnigk et al. [2003] employed the above-described variant of the watershed segmentation
      to the delineation of lung lobes in CT data.
      Ray et al. [2008] used the iterative watershed transform
      for hepatic tumor segmentation (and volumetry).*/
    </script>
  </body>
</html>
